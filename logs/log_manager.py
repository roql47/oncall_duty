#!/usr/bin/env python3
"""
ë¡œê·¸ ê´€ë¦¬ ìœ í‹¸ë¦¬í‹° ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python log_manager.py --help
    python log_manager.py --list-logs
    python log_manager.py --view django api 2024-01-15
    python log_manager.py --tail fastapi error
    python log_manager.py --clean --days 30
    python log_manager.py --stats
"""

import argparse
import json
import os
import sys
from datetime import datetime, timedelta
from pathlib import Path
import subprocess

# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ì˜ ë””ë ‰í† ë¦¬ ê²½ë¡œ
SCRIPT_DIR = Path(__file__).resolve().parent
LOGS_DIR = SCRIPT_DIR
PROJECT_ROOT = SCRIPT_DIR.parent

# ë¡œê·¸ íƒ€ì… ì •ì˜
LOG_BACKENDS = ['django', 'fastapi', 'system']
LOG_TYPES = {
    'django': ['access', 'api', 'error', 'debug'],
    'fastapi': ['access', 'api', 'chatbot', 'error', 'debug'],
    'system': ['startup', 'performance', 'security']
}

class LogManager:
    """ë¡œê·¸ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.logs_dir = LOGS_DIR
    
    def list_logs(self, backend=None, log_type=None, days=7):
        """ë¡œê·¸ íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
        print(f"\nğŸ“‹ ë¡œê·¸ íŒŒì¼ ëª©ë¡ (ìµœê·¼ {days}ì¼)")
        print("=" * 60)
        
        cutoff_date = datetime.now() - timedelta(days=days)
        
        for backend_name in LOG_BACKENDS:
            if backend and backend_name != backend:
                continue
                
            backend_dir = self.logs_dir / backend_name
            if not backend_dir.exists():
                continue
                
            print(f"\nğŸ”§ {backend_name.upper()} ë°±ì—”ë“œ:")
            
            for type_name in LOG_TYPES[backend_name]:
                if log_type and type_name != log_type:
                    continue
                    
                type_dir = backend_dir / type_name
                if not type_dir.exists():
                    continue
                
                print(f"  ğŸ“ {type_name}:")
                
                log_files = []
                for log_file in type_dir.glob("*.txt"):
                    file_date = datetime.fromtimestamp(log_file.stat().st_mtime)
                    if file_date >= cutoff_date:
                        file_size = log_file.stat().st_size
                        log_files.append((log_file, file_date, file_size))
                
                # ë‚ ì§œìˆœ ì •ë ¬
                log_files.sort(key=lambda x: x[1], reverse=True)
                
                if log_files:
                    for log_file, file_date, file_size in log_files:
                        size_mb = file_size / (1024 * 1024)
                        print(f"    ğŸ“„ {log_file.name} ({size_mb:.2f}MB, {file_date.strftime('%Y-%m-%d %H:%M')})")
                else:
                    print(f"    (ë¡œê·¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤)")
    
    def view_log(self, backend, log_type, date=None, lines=50):
        """ë¡œê·¸ íŒŒì¼ ë‚´ìš© ì¡°íšŒ"""
        if date is None:
            date = datetime.now().strftime('%Y-%m-%d')
        
        log_file = self.logs_dir / backend / log_type / f"{backend}_{log_type}_{date}.txt"
        
        if not log_file.exists():
            print(f"âŒ ë¡œê·¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {log_file}")
            return
        
        print(f"\nğŸ“– ë¡œê·¸ íŒŒì¼: {log_file}")
        print(f"ğŸ“… ë‚ ì§œ: {date}, ğŸ“Š ë¼ì¸ ìˆ˜: {lines}")
        print("=" * 80)
        
        try:
            with open(log_file, 'r', encoding='utf-8') as f:
                log_lines = f.readlines()
                
            # ë§ˆì§€ë§‰ Nì¤„ ì¶œë ¥
            start_idx = max(0, len(log_lines) - lines)
            for i, line in enumerate(log_lines[start_idx:], start_idx + 1):
                try:
                    # JSON ë¡œê·¸ì¸ ê²½ìš° íŒŒì‹±í•´ì„œ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥
                    log_data = json.loads(line.strip())
                    timestamp = log_data.get('timestamp', '')
                    level = log_data.get('level', '')
                    message = log_data.get('message', '')
                    
                    # ìƒ‰ìƒ ì½”ë“œ ì ìš©
                    color = self.get_level_color(level)
                    print(f"{i:4d} | {timestamp} | {color}{level:<7}{self.reset_color()} | {message}")
                    
                    # ì¶”ê°€ ì •ë³´ê°€ ìˆìœ¼ë©´ ì¶œë ¥
                    extra_info = []
                    if log_data.get('request_id'):
                        extra_info.append(f"ID: {log_data['request_id'][:8]}")
                    if log_data.get('ip_address'):
                        extra_info.append(f"IP: {log_data['ip_address']}")
                    if log_data.get('response_time'):
                        extra_info.append(f"Time: {log_data['response_time']}ms")
                    
                    if extra_info:
                        print(f"     | {' | '.join(extra_info)}")
                    
                except json.JSONDecodeError:
                    # JSONì´ ì•„ë‹Œ ì¼ë°˜ ë¡œê·¸ëŠ” ê·¸ëŒ€ë¡œ ì¶œë ¥
                    print(f"{i:4d} | {line.rstrip()}")
                    
        except Exception as e:
            print(f"âŒ ë¡œê·¸ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
    
    def tail_log(self, backend, log_type, lines=50):
        """ì‹¤ì‹œê°„ ë¡œê·¸ ëª¨ë‹ˆí„°ë§ (tail -f ê¸°ëŠ¥)"""
        today = datetime.now().strftime('%Y-%m-%d')
        log_file = self.logs_dir / backend / log_type / f"{backend}_{log_type}_{today}.txt"
        
        if not log_file.exists():
            print(f"âŒ ë¡œê·¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {log_file}")
            return
        
        print(f"ğŸ”„ ì‹¤ì‹œê°„ ë¡œê·¸ ëª¨ë‹ˆí„°ë§: {log_file}")
        print("ğŸ“ Ctrl+Cë¥¼ ëˆŒëŸ¬ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        print("=" * 80)
        
        try:
            # tail -f ëª…ë ¹ ì‹¤í–‰
            if os.name == 'nt':  # Windows
                subprocess.run(['powershell', 'Get-Content', str(log_file), '-Tail', str(lines), '-Wait'], check=True)
            else:  # Unix/Linux
                subprocess.run(['tail', '-f', '-n', str(lines), str(log_file)], check=True)
        except KeyboardInterrupt:
            print("\n\nâœ… ë¡œê·¸ ëª¨ë‹ˆí„°ë§ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        except subprocess.CalledProcessError as e:
            print(f"âŒ tail ëª…ë ¹ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
    
    def clean_logs(self, days=30, dry_run=False):
        """ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼ ì •ë¦¬"""
        cutoff_date = datetime.now() - timedelta(days=days)
        
        print(f"ğŸ—‘ï¸ {days}ì¼ ì´ì „ ë¡œê·¸ íŒŒì¼ ì •ë¦¬ {'(ì‹œë®¬ë ˆì´ì…˜)' if dry_run else ''}")
        print(f"ğŸ“… ê¸°ì¤€ ë‚ ì§œ: {cutoff_date.strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 60)
        
        total_deleted = 0
        total_size_freed = 0
        
        for backend_name in LOG_BACKENDS:
            backend_dir = self.logs_dir / backend_name
            if not backend_dir.exists():
                continue
            
            print(f"\nğŸ”§ {backend_name.upper()} ë°±ì—”ë“œ:")
            
            for type_name in LOG_TYPES[backend_name]:
                type_dir = backend_dir / type_name
                if not type_dir.exists():
                    continue
                
                print(f"  ğŸ“ {type_name}:")
                
                deleted_count = 0
                size_freed = 0
                
                for log_file in type_dir.glob("*.txt"):
                    file_date = datetime.fromtimestamp(log_file.stat().st_mtime)
                    
                    # ì—ëŸ¬ ë¡œê·¸ì™€ ë³´ì•ˆ ë¡œê·¸ëŠ” ë” ì˜¤ë˜ ë³´ê´€
                    if type_name in ['error', 'security']:
                        actual_cutoff = datetime.now() - timedelta(days=90)
                    else:
                        actual_cutoff = cutoff_date
                    
                    if file_date < actual_cutoff:
                        file_size = log_file.stat().st_size
                        
                        print(f"    âŒ ì‚­ì œ ëŒ€ìƒ: {log_file.name} ({file_size/(1024*1024):.2f}MB, {file_date.strftime('%Y-%m-%d')})")
                        
                        if not dry_run:
                            log_file.unlink()
                        
                        deleted_count += 1
                        size_freed += file_size
                
                if deleted_count > 0:
                    print(f"    âœ… {deleted_count}ê°œ íŒŒì¼ ì‚­ì œë¨ ({size_freed/(1024*1024):.2f}MB)")
                    total_deleted += deleted_count
                    total_size_freed += size_freed
                else:
                    print(f"    (ì‚­ì œí•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤)")
        
        print(f"\nğŸ“Š ì´ ì •ë¦¬ ê²°ê³¼:")
        print(f"  ğŸ“„ ì‚­ì œëœ íŒŒì¼: {total_deleted}ê°œ")
        print(f"  ğŸ’¾ í™•ë³´ëœ ìš©ëŸ‰: {total_size_freed/(1024*1024):.2f}MB")
    
    def get_stats(self):
        """ë¡œê·¸ í†µê³„ ì¡°íšŒ"""
        print("\nğŸ“Š ë¡œê·¸ ì‹œìŠ¤í…œ í†µê³„")
        print("=" * 60)
        
        total_files = 0
        total_size = 0
        backend_stats = {}
        
        for backend_name in LOG_BACKENDS:
            backend_dir = self.logs_dir / backend_name
            if not backend_dir.exists():
                continue
            
            backend_stats[backend_name] = {'files': 0, 'size': 0, 'types': {}}
            
            for type_name in LOG_TYPES[backend_name]:
                type_dir = backend_dir / type_name
                if not type_dir.exists():
                    continue
                
                type_files = 0
                type_size = 0
                
                for log_file in type_dir.glob("*.txt"):
                    file_size = log_file.stat().st_size
                    type_files += 1
                    type_size += file_size
                
                backend_stats[backend_name]['types'][type_name] = {
                    'files': type_files,
                    'size': type_size
                }
                backend_stats[backend_name]['files'] += type_files
                backend_stats[backend_name]['size'] += type_size
            
            total_files += backend_stats[backend_name]['files']
            total_size += backend_stats[backend_name]['size']
        
        # ì „ì²´ í†µê³„ ì¶œë ¥
        print(f"ğŸ“„ ì´ ë¡œê·¸ íŒŒì¼: {total_files}ê°œ")
        print(f"ğŸ’¾ ì´ ì‚¬ìš© ìš©ëŸ‰: {total_size/(1024*1024):.2f}MB")
        
        # ë°±ì—”ë“œë³„ í†µê³„
        for backend_name, stats in backend_stats.items():
            if stats['files'] > 0:
                print(f"\nğŸ”§ {backend_name.upper()}:")
                print(f"  ğŸ“„ íŒŒì¼ ìˆ˜: {stats['files']}ê°œ")
                print(f"  ğŸ’¾ ìš©ëŸ‰: {stats['size']/(1024*1024):.2f}MB")
                
                for type_name, type_stats in stats['types'].items():
                    if type_stats['files'] > 0:
                        print(f"    ğŸ“ {type_name}: {type_stats['files']}ê°œ, {type_stats['size']/(1024*1024):.2f}MB")
    
    def get_level_color(self, level):
        """ë¡œê·¸ ë ˆë²¨ì— ë”°ë¥¸ ìƒ‰ìƒ ì½”ë“œ ë°˜í™˜"""
        colors = {
            'DEBUG': '\033[36m',    # ì²­ë¡ìƒ‰
            'INFO': '\033[32m',     # ë…¹ìƒ‰
            'WARNING': '\033[33m',  # ë…¸ë€ìƒ‰
            'ERROR': '\033[31m',    # ë¹¨ê°„ìƒ‰
            'CRITICAL': '\033[35m', # ë§ˆì  íƒ€
        }
        return colors.get(level, '\033[0m')
    
    def reset_color(self):
        """ìƒ‰ìƒ ë¦¬ì…‹"""
        return '\033[0m'

def main():
    parser = argparse.ArgumentParser(description='ë¡œê·¸ ê´€ë¦¬ ìœ í‹¸ë¦¬í‹°')
    
    # ì„œë¸Œì»¤ë§¨ë“œ ì„¤ì •
    subparsers = parser.add_subparsers(dest='command', help='ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹')
    
    # list ëª…ë ¹
    list_parser = subparsers.add_parser('list', help='ë¡œê·¸ íŒŒì¼ ëª©ë¡ ì¡°íšŒ')
    list_parser.add_argument('--backend', choices=LOG_BACKENDS, help='ë°±ì—”ë“œ í•„í„°')
    list_parser.add_argument('--type', help='ë¡œê·¸ íƒ€ì… í•„í„°')
    list_parser.add_argument('--days', type=int, default=7, help='ì¡°íšŒí•  ì¼ìˆ˜ (ê¸°ë³¸: 7ì¼)')
    
    # view ëª…ë ¹
    view_parser = subparsers.add_parser('view', help='ë¡œê·¸ íŒŒì¼ ë‚´ìš© ì¡°íšŒ')
    view_parser.add_argument('backend', choices=LOG_BACKENDS, help='ë°±ì—”ë“œ ì„ íƒ')
    view_parser.add_argument('type', help='ë¡œê·¸ íƒ€ì… ì„ íƒ')
    view_parser.add_argument('--date', help='ë‚ ì§œ (YYYY-MM-DD í˜•ì‹, ê¸°ë³¸: ì˜¤ëŠ˜)')
    view_parser.add_argument('--lines', type=int, default=50, help='ì¶œë ¥í•  ë¼ì¸ ìˆ˜ (ê¸°ë³¸: 50)')
    
    # tail ëª…ë ¹
    tail_parser = subparsers.add_parser('tail', help='ì‹¤ì‹œê°„ ë¡œê·¸ ëª¨ë‹ˆí„°ë§')
    tail_parser.add_argument('backend', choices=LOG_BACKENDS, help='ë°±ì—”ë“œ ì„ íƒ')
    tail_parser.add_argument('type', help='ë¡œê·¸ íƒ€ì… ì„ íƒ')
    tail_parser.add_argument('--lines', type=int, default=50, help='ì´ˆê¸° ì¶œë ¥í•  ë¼ì¸ ìˆ˜ (ê¸°ë³¸: 50)')
    
    # clean ëª…ë ¹
    clean_parser = subparsers.add_parser('clean', help='ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼ ì •ë¦¬')
    clean_parser.add_argument('--days', type=int, default=30, help='ë³´ê´€í•  ì¼ìˆ˜ (ê¸°ë³¸: 30ì¼)')
    clean_parser.add_argument('--dry-run', action='store_true', help='ì‹¤ì œë¡œ ì‚­ì œí•˜ì§€ ì•Šê³  ì‹œë®¬ë ˆì´ì…˜ë§Œ ì‹¤í–‰')
    
    # stats ëª…ë ¹
    stats_parser = subparsers.add_parser('stats', help='ë¡œê·¸ í†µê³„ ì¡°íšŒ')
    
    args = parser.parse_args()
    
    if args.command is None:
        parser.print_help()
        return
    
    log_manager = LogManager()
    
    try:
        if args.command == 'list':
            log_manager.list_logs(args.backend, args.type, args.days)
        
        elif args.command == 'view':
            log_manager.view_log(args.backend, args.type, args.date, args.lines)
        
        elif args.command == 'tail':
            log_manager.tail_log(args.backend, args.type, args.lines)
        
        elif args.command == 'clean':
            log_manager.clean_logs(args.days, args.dry_run)
        
        elif args.command == 'stats':
            log_manager.get_stats()
            
    except KeyboardInterrupt:
        print("\n\nâœ… ì‘ì—…ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

if __name__ == '__main__':
    main() 